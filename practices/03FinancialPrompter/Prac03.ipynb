{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Financial News Scrapper\n",
    "\n",
    "이번 실습에서는 Google News로부터 미국 종목의 티커를 검색해 투자 조언을 생성하는 LLM을 생성하는 것으로 한다."
   ],
   "id": "c1224c047761dbdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Google Search function\n",
    "\n",
    "Google News에서 내용을 파싱하는 함수가 다음과 같이 존재한다. 이를 실행하기 위해서는 다음과 같은 라이브러리가 필요하다.\n",
    "\n",
    "- `Selenium` : pip install selenium\n",
    "- `webdriver_manager` : pip3 install webdriver_manager\n",
    "- `Newspaper` : pip3 install newspaper3k\n",
    "- `lxml_html_clean` : pip install lxml_html_clean"
   ],
   "id": "10490d34d39e9c20"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# import package\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from selenium import webdriver"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def search_from_google(asset:str, page_nums:int) -> pd.DataFrame :\n",
    "    '''\n",
    "    google news로부터 검색을 한 뒤, selenium을 통해 뉴스 데이터들을 가져옵니다.\n",
    "    :param asset: 검색할 자산\n",
    "    :param page_nums: 뉴스를 검색할 총 페이지의 수\n",
    "    :return: news data가 들어있는 DataFrame\n",
    "    '''\n",
    "    keyword = f'{asset} buying reason'\n",
    "    news_df = pd.DataFrame()\n",
    "    \n",
    "    # selenium headless mode\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    for page_num in range(0, page_nums-1):\n",
    "        # google news crawling\n",
    "        url = f'https://www.google.com/search?q={keyword}&sca_esv=1814fa2a4600643d&tbas=0&tbs=qdr:m&tbm=nws&ei=rE3pZeLxNeHX1e8PpdOcMA&start={page_num}&sa=N&ved=2ahUKEwji9-zrsuGEAxXha_UHHaUpBwYQ8tMDegQIBBAE&biw=2560&bih=1313&dpr=1'\n",
    "        req = requests.get(url)\n",
    "        content = req.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "        # last page check\n",
    "        if soup.select('div.BNeawe.vvjwJb') == []: break\n",
    "    \n",
    "        title_list = [t.text for t in soup.select('div.BNeawe.vvjwJb')]  # title\n",
    "        url_list = []\n",
    "    \n",
    "        # url\n",
    "        for u in soup.select('a'):\n",
    "            for t in title_list:\n",
    "                if t in u.text:\n",
    "                    temp_url = urllib.parse.unquote(u['href'])\n",
    "                    temp_url = re.findall('http\\S+&sa',temp_url)[0][:-3]\n",
    "                    url_list.append(temp_url)\n",
    "    \n",
    "        # article\n",
    "        for ind, news_url in enumerate(url_list):\n",
    "            try:\n",
    "                article = Article(url=news_url)\n",
    "                article.download()\n",
    "                article.parse()    \n",
    "                news_article = article.text\n",
    "            except:  # ssl error\n",
    "                driver.get(news_url)\n",
    "                article.download(input_html=driver.page_source)\n",
    "                article.parse()\n",
    "                news_article = article.text\n",
    "    \n",
    "            news_df = pd.concat([news_df, pd.DataFrame([[title_list[ind], news_article, news_url]])])\n",
    "    \n",
    "        news_df[0] = news_df[0].apply(lambda x: re.sub('\\s+',' ',x))\n",
    "        news_df = news_df.reset_index(drop=True)\n",
    "    \n",
    "    news_df.columns = ['Title','Contents','URL']\n",
    "    \n",
    "    return news_df"
   ],
   "id": "b3ff3806fb76cf6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "search_result = search_from_google('META', 3)",
   "id": "920542e426ce8352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 출력 결과의 확인\n",
    "search_result.head()"
   ],
   "id": "587a7c18b9cde3e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Prompt Template\n",
    "\n",
    "1번에서 수집된 뉴스 데이터의 투자 포인트와 매수, 매도를 결정하는 함수 `get_investing_point_llm`과 `get_side_from_gpt`를 아래의 규칙에 따라서 정의한다.\n",
    "\n",
    "#### `get_investing_point_llm`\n",
    "- 복잡한 과업을 하위 과업으로 분할하여 지시한다.\n",
    "- 산출한 결과에 대해서 정당화 요청을 수행한다.\n",
    "- 여러 개의 결과를 출력해 가장 적합한 것을 선택하도록 한다.\n",
    "- 지침을 반복한다.\n",
    "- 구분선을 사용하여 예시를 제공한다.\n",
    "- 만약 기사 내용이 기업과 관련이 없는 경우, '관련 없음'의 내용이 채워지도록 지시한다.\n",
    "\n",
    "#### `get_side_from_gpt`\n",
    "\n",
    "- 지침을 반복함으로써, Sell, Buy, 혹은 Hold로 세 개의 label만 나오도록 강제한다.\n",
    "\n",
    "혹은, 1번의 기능과 결합하여 독립적인 Class로 디자인하여도 무방하다."
   ],
   "id": "d0867ba0856aca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import openai\n",
    "\n",
    "CONFIG_PATH = '' # write your api key path\n",
    "\n",
    "# with open(\"../config/open_ai.key\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     config = lines[0].strip()\n",
    "# \n",
    "# openai.api_key = config\n",
    "# os.environ[\"OPENAI_API_KEY\"] = config"
   ],
   "id": "f51562c38e2d0db1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_investing_point_llm(\n",
    "        asset:str,\n",
    "        news_df: pd.DataFrame,\n",
    "        temperature:float,\n",
    "        lm_model:str,\n",
    "    ) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    뉴스가 포함된 데이터프레임으로부터 새로운 열 ['investing_point']를 생성하여 GPT로부터 생성된 투자 포인트를 저장하는 함수입니다.\n",
    "    :param asset: 자산의 티커입니다.\n",
    "    :param news_df: 1번으로부터 생성된 데이터프레임을 넣으면 됩니다.\n",
    "    :param temperature: LLM Model의 temperature를 지정하는 parameter입니다.\n",
    "    :param lm_model: 사용할 LLM Model을 지정하는 parameter입니다.\n",
    "    :return: news_df에 투자 포인트의 컬럼이 추가된 pd.DataFrame형태의 data frame입니다.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def get_side_from_gpt(\n",
    "        news_df: pd.DataFrame,\n",
    "        temperature:float,\n",
    "        lm_model:str,\n",
    "    ) -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    뉴스가 포함된 데이터프레임으로부터 새로운 열 ['investing_point']를 사용하여 GPT로부터 생성된 투자 방향(position)을 생성하여 저장하는 함수입니다.\n",
    "    :param news_df: get_investing_point_llm 함수로부터 출력된 결과물을 넣습니다.\n",
    "    :param temperature: LLM Model의 temperature를 지정하는 parameter입니다.\n",
    "    :param lm_model: 사용할 LLM Model을 지정하는 parameter입니다.\n",
    "    :return: news_df에 side의 컬럼이 추가된 pd.DataFrame형태의 data frame입니다.\n",
    "    \"\"\"\n",
    "    pass"
   ],
   "id": "5d0d0f64c8d1ade",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Agent GPT\n",
    "\n",
    "2번으로부터 생성된 투자 포인트를 결합하여 다음의 과업을 수행하는 일련의 Agent 모델을 생성한다\n",
    "\n",
    "- 생성된 투자포인트의 모음으로부터 실제로 발생한 사실과 의견을 구분하는 Agent를 생성한다.\n",
    "- 향후 전망을 전문적으로 수행하는 에이전트를 생성해 기업 전망을 생성하도록 한다.\n",
    "- side로부터 정보를 불러와, 최종적으로 매수와 매도를 판단하는 Agent를 생성한다.\n",
    "\n",
    "아래의 규칙에 따라 Agent를 생성하고 결과물을 출력해 보자.\n",
    "\n",
    "- 산출한 결과에 대해서 정당화 요청을 수행한다.\n",
    "- 여러 개의 결과를 출력해 가장 적합한 것을 선택하도록 한다.\n",
    "- 번역한 내용 자체는 출력하지 않도록 반복 요청한다.\n",
    "- 사실을 출력하기 위한 Agent는 예시를 제공하여 (ex: eps가 3% 시장 컨센서스를 상회하였음) 올바른 답을 내도록 유도한다."
   ],
   "id": "e40c041808313acd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "15022f4b18248bc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. ReAct\n",
    "\n",
    "3번으로부터 보고받은 결과물에 대해, `SerpAPI`를 사용하여 발생한 사실들에 대해 예상되는 효과를 검색하여 추론하도록 설계한다. "
   ],
   "id": "5240e98df7959214"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "SERP_API_KEY = '<KEY>' # enter your SerpAPI Key!\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name = 'gpt-4o-mini',\n",
    "    api_key = openai.api_key,\n",
    "    \n",
    ")\n",
    "# tools = load_tools(\n",
    "#     ['serpapi'],\n",
    "#     serpapi_api_key = SERP_API_KEY\n",
    "# )\n",
    "# prompt = hub.pull('hwchase17/react')"
   ],
   "id": "74a17684b2a1efaf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
